{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Desenvolvimento de Data-Driven Apps com Python**\n",
    "# **Teste de Performance 2 (TP2)**\n",
    "\n",
    "## Instituto Infnet - Rafael Dottori de Oliveira\n",
    "\n",
    "### 21/11/2024\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado\n",
    "\n",
    "*Este Teste de Performance (TP) é composto por 10 questões, nas quais você deve demonstrar habilidades práticas no desenvolvimento de aplicações simples utilizando FastAPI ou LangChain com modelos de linguagem (LLMs).*\n",
    "\n",
    "*• Orientações:*\n",
    "\n",
    "*O código deve ser disponibilizado no GitHub, seguindo boas práticas de projeto.*\n",
    "\n",
    "*Em questões relacionadas à arquitetura, os alunos devem fornecer diagramas que representem a estrutura da aplicação.*\n",
    "\n",
    "*Nas questões de comparação, as respostas devem ser organizadas em formato de tabela.*\n",
    "\n",
    "\n",
    "*• Critérios de Avaliação:*\n",
    "\n",
    "*Organização e clareza do código.*\n",
    "\n",
    "*Uso de boas práticas de desenvolvimento.*\n",
    "\n",
    "*Implementação correta e funcional dos aplicativos.*\n",
    "\n",
    "*Explicações concisas com diagramas ou tabelas conforme solicitado.*\n",
    "\n",
    "*• Entrega Final:*\n",
    "\n",
    "*Todo o código deve ser submetido no GitHub, organizado em repositórios separados para cada questão prática 1-2 (Parte 1) e 1-2-3 (Parte 2).*\n",
    "\n",
    "*Incluir um arquivo README.md com instruções detalhadas sobre como executar cada aplicação.*\n",
    "\n",
    "*As explicações e diagramas para as questões 3-4 (Parte 1) e 4-5-6 (Parte 2) devem ser enviadas em um arquivo PDF, juntamente com os diagramas e tabelas solicitados.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Parte 1: FastAPI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Questão 1**\n",
    "\n",
    "*Crie uma aplicação simples em FastAPI que utilize o modelo GPT-2 da HuggingFace para gerar textos a partir de uma entrada fornecida via requisição HTTP.*\n",
    "\n",
    "*• O aplicativo deve:*\n",
    "\n",
    "*Receber uma frase de entrada como JSON.*\n",
    "\n",
    "*Utilizar a biblioteca transformers do HuggingFace para gerar um texto de saída.*\n",
    "\n",
    "*Retornar o texto gerado em uma resposta HTTP.*\n",
    "\n",
    "*• O que é esperado:*\n",
    "\n",
    "*O aplicativo deve gerar uma continuação de texto a partir de uma frase de entrada e retornar a resposta formatada como JSON.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos organizar os códigos das aplicações em três arquivos .py separados: o com os formatos dos dados enviados utilizando BaseModel, o com as rotas das páginas adicionais da API (por exemplo, localhost:8000/pagina_extra) e um arquivo principal para rodar a API e apontar para as rotas criadas.\n",
    "\n",
    "Desse jeito, é mais fácil adicionar e manter novas rotas e modelos.\n",
    "\n",
    "Normalmente escreveríamos o conteúdo diretamente nos arquivos .py. Ao longo do trabalho utilizaremos comandos de *cell magic* para poder exibir no próprio *notebook* o conteúdo desses arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./app_fastapi/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./app_fastapi/models.py\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ModeloTexto(BaseModel):\n",
    "    texto: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./app_fastapi/routers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./app_fastapi/routers.py\n",
    "\n",
    "from fastapi import APIRouter\n",
    "from .models import ModeloTexto\n",
    "from transformers import pipeline\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "\n",
    "@router.post('/gerar_texto/')\n",
    "async def gerar_texto(body: ModeloTexto):\n",
    "    modelo = pipeline('text-generation', model='gpt2')\n",
    "    resposta = modelo(body.texto)\n",
    "    return {'resposta': resposta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./app_fastapi/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./app_fastapi/main.py\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from .routers import router\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "app.include_router(router, prefix='/chat')\n",
    "\n",
    "\n",
    "@app.get('/')\n",
    "async def raiz():\n",
    "    return {'mensagem': 'Página raiz'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testarmos a aplicação, utilizamos o uvicorn via terminal:\n",
    "\n",
    "> uvicorn app_fastapi.main:app\n",
    "\n",
    "E com o httpie, podemos testar os métodos de HTTP, como acessar a página raiz da aplicação:\n",
    "\n",
    "> http GET localhost:8000/\n",
    "\n",
    "Abaixo, testaremos o modelo GPT-2 do HuggingFace com uma pergunta qualquer\n",
    "\n",
    "> http POST localhost:8000/chat/gerar_texto/ texto='How are you?'\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"generated_text\": \"How are you? The first thing you notice is that your body's expression changes with different body types. You don't grow taller, and your mouth doesn't grow longer. Your facial hair grows longer. You don't sweat more. The facial hair\"\n",
    "    }\n",
    "]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Questão 2**\n",
    "\n",
    "*Crie um aplicativo FastAPI que utiliza o modelo de tradução Helsinki-NLP/opus-mt-en-fr da HuggingFace para traduzir textos do inglês para o francês.*\n",
    "\n",
    "*• A aplicação deve:*\n",
    "\n",
    "*Receber um texto em inglês via uma requisição HTTP.*\n",
    "\n",
    "*Traduzir o texto para o francês utilizando o modelo de tradução.*\n",
    "\n",
    "*Retornar o texto traduzido em uma resposta JSON.*\n",
    "\n",
    "*• O que é esperado:*\n",
    "\n",
    "*A API deve receber um texto em inglês e retornar sua tradução para o francês, processando tanto frases curtas quanto textos mais longos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como explicamos anteriormente, podemos melhorar a aplicação sem mexer no arquivo main.py.\n",
    "\n",
    "Vamos adicionar mais um modelo e uma nova rota para gerar as traduções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./app_fastapi/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./app_fastapi/models.py\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ModeloTexto(BaseModel):\n",
    "    texto: str\n",
    "\n",
    "\n",
    "class ModeloTraducao(BaseModel):\n",
    "    texto: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./app_fastapi/routers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./app_fastapi/routers.py\n",
    "\n",
    "from fastapi import APIRouter\n",
    "from .models import ModeloTexto, ModeloTraducao\n",
    "from transformers import pipeline\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "\n",
    "@router.post('/gerar_texto/')\n",
    "async def gerar_texto(body: ModeloTexto):\n",
    "    modelo = pipeline('text-generation', model='gpt2')\n",
    "    resposta = modelo(body.texto)\n",
    "    return {'resposta': resposta}\n",
    "\n",
    "\n",
    "@router.post('/traduzir/')\n",
    "async def traduzir_texto(body: ModeloTraducao):\n",
    "    modelo = pipeline('translation_en_to_fr', model='Helsinki-NLP/opus-mt-en-fr', max_length=256)\n",
    "    resposta = modelo(body.texto)\n",
    "    return {'resposta': resposta}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rodamos novamente nosso aplicativo via uvicorn e podemos usar o método POST para testarmos as traduções.\n",
    "\n",
    "> http POST localhost:8000/chat/traduzir/ texto='Translate this text, please.'\n",
    "\n",
    "HTTP/1.1 200 OK\n",
    "content-length: 61\n",
    "content-type: application/json\n",
    "date: Tue, 12 Nov 2024 20:53:59 GMT\n",
    "server: uvicorn\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"translation_text\": \"Traduire ce texte, s'il vous plaît.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "> http POST localhost:8000/chat/traduzir/ texto=\"Today's the debut of our new play. I've told everyone to break a leg!\"\n",
    "\n",
    "HTTP/1.1 200 OK\n",
    "content-length: 139\n",
    "content-type: application/json\n",
    "date: Wed, 13 Nov 2024 17:29:29 GMT\n",
    "server: uvicorn\n",
    "\n",
    "{\n",
    "    \"resposta\": [\n",
    "        {\n",
    "            \"translation_text\": \"Aujourd'hui c'est le début de notre nouvelle pièce. J'ai dit à tout le monde de casser une jambe !\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Questão 3**\n",
    "\n",
    "*Com base na API desenvolvida na Questão 2 (Parte1), explique as principais limitações do modelo de tradução utilizado.*\n",
    "\n",
    "*• Enumere e discuta:*\n",
    "\n",
    "*Limitações quanto à precisão da tradução.*\n",
    "\n",
    "*Desafios de tempo de resposta e desempenho em grande escala.*\n",
    "\n",
    "*Restrições de custo e escalabilidade.*\n",
    "\n",
    "*Limitações na tradução de gírias, expressões idiomáticas ou linguagem de contexto.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A aplicação criada demostrou algumas limitações, principalmente no tempo de resposta. Os testes foram feitos pelo terminal do Visual Studio Code, utilizando as bibliotecas uvicorn e httpie.\n",
    "\n",
    "A princípio, as traduções geradas do inglês para o francês foram satisfatórias, incluindo palavras que possam ter mais de um significado (\"play\"). Ao inserir a expressão idiomática \"break a leg\", o tradutor a traduziu literalmente.\n",
    "\n",
    "Se pensássemos nessa mesma aplicação em um volume maior de dados, teríamos ainda mais problemas com as traduções, com o custo do treinamento, tempo de respostas, etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Questão 4**\n",
    "\n",
    "*Com base no modelo GPT-2 utilizado na Questão 1 (Parte 1), explique as principais limitações do modelo no contexto da geração de texto.*\n",
    "\n",
    "*• Discuta:*\n",
    "\n",
    "*A coerência do texto gerado.*\n",
    "\n",
    "*Possíveis falhas ou incoerências geradas por LLMs.*\n",
    "\n",
    "*Desempenho e questões de latência.*\n",
    "\n",
    "*Limitações na geração de conteúdo apropriado.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No próprio exemplo que visualizamos no primeiro exercício já conseguimos apontar limitações do modelo:\n",
    "\n",
    "*How are you? The first thing you notice is that your body's expression changes with different body types. You don't grow taller, and your mouth doesn't grow longer. Your facial hair grows longer. You don't sweat more. The facial hair*\n",
    "\n",
    "Provavelmente por conta de um treinamento limitado, o modelo tem dificuldade de entender contextos e começa a mudar de assunto ao longo da resposta. Além disso, a resposta nem segue um sentido lógico da nossa realidade (alucinações de uma LLM).\n",
    "\n",
    "Essa falta de contexto também poderia levar a respostas que não seguem o formato ou tom pedido — por exemplo, responder uma pergunta com outra pergunta, responder uma pergunta séria com uma piada, etc.\n",
    "\n",
    "Para resolver tais problemas, esbarramos também em limitações técnicas ou de custo. Seria necessário treinar o modelo com uma base muito maior de dados e com ajustes e parâmetros que fizessem o modelo entender contextos específicos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Parte 2: LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Questão 1**\n",
    "\n",
    "*Desenvolva um protótipo utilizando LangChain que simule um chatbot simples com Fake LLM.*\n",
    "\n",
    "*• A aplicação deve:*\n",
    "\n",
    "*Receber um input de texto via FastAPI.*\n",
    "\n",
    "*Retornar uma resposta simulada pelo Fake LLM.*\n",
    "\n",
    "\n",
    "*• O que é esperado:*\n",
    "\n",
    "*O protótipo deve simular um chatbot básico que responde a perguntas pré-definidas.*\n",
    "\n",
    "*A arquitetura deve ser simples, e você deve explicar a importância de usar Fake LLM para testes rápidos.*\n",
    "\n",
    "*Desenhe um diagrama simples da arquitetura do aplicativo, detalhando as principais etapas do fluxo de dados.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos montar uma nova aplicação seguindo a estrutura da anterior, com um arquivo para os modelos, um para as rotas e outro principal para rodarmos a aplicação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./app_langchain/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./app_langchain/models.py\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ModeloPergunta(BaseModel):\n",
    "    pergunta: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./app_langchain/routers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./app_langchain/routers.py\n",
    "\n",
    "from fastapi import APIRouter\n",
    "from .models import ModeloPergunta\n",
    "from langchain_community.llms import FakeListLLM\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "# Instanciamos o modelo fora da função correspondente pois foi a forma de fazê-lo progredir nas respostas da lista\n",
    "modelo_falso = FakeListLLM(responses=[\n",
    "                                    'Olá, eu sou um robô!',\n",
    "                                    'Estou bem, e você?',\n",
    "                                    'Não sei te dizer...',\n",
    "                                    'Essa é a última resposta da lista.'\n",
    "                                    ]\n",
    "                    )\n",
    "\n",
    "\n",
    "@router.post('/conversar/')\n",
    "async def conversa_falsa(body: ModeloPergunta):\n",
    "    return {'resposta': modelo_falso.invoke(body.pergunta)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./app_langchain/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./app_langchain/main.py\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from .routers import router\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "app.include_router(router)\n",
    "\n",
    "\n",
    "@app.get('/')\n",
    "async def raiz():\n",
    "    return {'mensagem': 'Página raiz'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer requisições POST seguidas para observarmos o Fake LLM utilizando as respostas falsas em sequência.\n",
    "\n",
    "> http POST localhost:8000/conversar/ pergunta='Olá'\n",
    "\n",
    "HTTP/1.1 200 OK\n",
    "content-length: 34\n",
    "date: Wed, 13 Nov 2024 19:29:33 GMT\n",
    "server: uvicorn\n",
    "\n",
    "{\n",
    "    \"resposta\": \"Olá, eu sou um robô!\"\n",
    "}\n",
    "\n",
    "> http POST localhost:8000/conversar/ pergunta='Olá'\n",
    "\n",
    "HTTP/1.1 200 OK\n",
    "content-length: 34\n",
    "date: Wed, 13 Nov 2024 19:29:37 GMT\n",
    "server: uvicorn\n",
    "\n",
    "{\n",
    "    \"resposta\": \"Estou bem, e você?\"\n",
    "}\n",
    "\n",
    "> http POST localhost:8000/conversar/ pergunta='Uma pergunta qualquer'\n",
    "\n",
    "HTTP/1.1 200 OK\n",
    "content-length: 35\n",
    "content-type: application/json\n",
    "date: Wed, 13 Nov 2024 19:29:46 GMT\n",
    "server: uvicorn\n",
    "\n",
    "{\n",
    "    \"resposta\": \"Não sei te dizer...\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao utilizarmos Fake LLM para os testes, temos respostas obtidas de maneira rápida — já que o modelo não está gerando uma resposta de verdade — e sem os gastos por resposta que alguns modelos cobram (como os da OpenAI).\n",
    "\n",
    "Abaixo temos um diagrama da arquitetura do aplicativo:\n",
    "\n",
    "![arquitetura](TP2%20-%20Arquitetura.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Questão 2**\n",
    "\n",
    "*Desenvolva um aplicativo que utilize LangChain para integrar a API da OpenAI.*\n",
    "\n",
    "*• O aplicativo deve:*\n",
    "\n",
    "*Receber um texto em inglês via FastAPI.*\n",
    "\n",
    "*Traduzir o texto para o francês utilizando um modelo da OpenAI via LangChain.*\n",
    "\n",
    "*Retornar o texto traduzido em uma resposta JSON.*\n",
    "\n",
    "*• O que é esperado:*\n",
    "\n",
    "*O aplicativo deve funcionar como uma API de tradução, semelhante à questão 2 (Parte 1), mas utilizando a OpenAI via LangChain.*\n",
    "\n",
    "*A aplicação deve gerenciar as chamadas à API da OpenAI e retornar a tradução com baixa latência.*\n",
    "\n",
    "*Forneça um diagrama da arquitetura da aplicação, destacando os componentes principais, como FastAPI, LangChain, e OpenAI.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Não foi possível utilizar nenhuma versão gratuita da API da OpenAI. Portanto, nos exercícios que a pedem, utilizei a API do Google para rodar o modelo Gemini em seu lugar.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./app_langchain/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./app_langchain/models.py\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ModeloPergunta(BaseModel):\n",
    "    pergunta: str\n",
    "\n",
    "\n",
    "class ModeloTraducao(BaseModel):\n",
    "    texto: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./app_langchain/routers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./app_langchain/routers.py\n",
    "\n",
    "from fastapi import APIRouter\n",
    "from .models import ModeloPergunta, ModeloTraducao\n",
    "from langchain_community.llms import FakeListLLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Instanciamos o modelo fora da função correspondente pois foi a forma de fazê-lo progredir nas respostas da lista\n",
    "modelo_falso = FakeListLLM(responses=[\n",
    "                                    'Olá, eu sou um robô!',\n",
    "                                    'Estou bem, e você?',\n",
    "                                    'Não sei te dizer...',\n",
    "                                    'Essa é a última resposta da lista.'\n",
    "                                    ]\n",
    "                    )\n",
    "\n",
    "\n",
    "@router.post('/conversar/')\n",
    "async def conversa_falsa(body: ModeloPergunta):\n",
    "    return {'resposta': modelo_falso.invoke(body.pergunta)}\n",
    "\n",
    "\n",
    "@router.post('/traduzir/')\n",
    "async def traduzir_texto(body: ModeloTraducao):\n",
    "    modelo_traducao = ChatGoogleGenerativeAI(model='gemini-1.5-flash', api_key=os.getenv('GEMINI_KEY'))\n",
    "    resposta = ChatPromptTemplate([\n",
    "        ('system', 'Translate the english texts to french.'),\n",
    "        ('user', 'Translate the following text: {texto}')\n",
    "    ])\n",
    "\n",
    "    return  {'resposta': modelo_traducao.invoke(resposta.format_messages(texto=body.texto)).content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> http POST localhost:8000/traduzir/ texto='Please, translate this text.'\n",
    "\n",
    "HTTP/1.1 200 OK\n",
    "content-length: 45\n",
    "content-type: application/json\n",
    "date: Wed, 13 Nov 2024 20:37:40 GMT\n",
    "server: uvicorn\n",
    "\n",
    "{\n",
    "    \"resposta\": \"Veuillez traduire ce texte. \\n\"\n",
    "}\n",
    "\n",
    "> http POST localhost:8000/traduzir/ texto='Today is wednesday. Is such  a nice day outside.'\n",
    "\n",
    "HTTP/1.1 200 OK\n",
    "content-length: 77\n",
    "content-type: application/json\n",
    "date: Wed, 13 Nov 2024 20:38:40 GMT\n",
    "server: uvicorn\n",
    "\n",
    "{\n",
    "    \"resposta\": \"Aujourd'hui, c'est mercredi. Il fait tellement beau dehors. \\n\"  \n",
    "}\n",
    "\n",
    "![arquitetura_gemini](Diagrama%20-%20Arquitetura%20Gemini.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Questão 3**\n",
    "\n",
    "*Crie uma API semelhante à Questão 2 (Parte 2), mas que utilize o modelo Helsinki-NLP/opus-mt-en-de da HuggingFace para traduzir textos do inglês para o alemão.*\n",
    "\n",
    "*• A aplicação deve:*\n",
    "\n",
    "*Receber um texto em inglês via FastAPI.*\n",
    "\n",
    "*Utilizar o LangChain para gerenciar as chamadas ao modelo HuggingFace.*\n",
    "\n",
    "*Retornar o texto traduzido para o alemão como resposta JSON.*\n",
    "\n",
    "*• O que é esperado:*\n",
    "\n",
    "*O objetivo é que a aplicação funcione de maneira semelhante às Questões 2 (Parte 1) e 2 (Parte 2), mas desta vez integrando LangChain com HuggingFace.*\n",
    "\n",
    "*O modelo a ser utilizado deve ser o Helsinki-NLP/opus-mt-en-de.*\n",
    "\n",
    "*Forneça um diagrama detalhado da arquitetura da aplicação, destacando as interações entre FastAPI, LangChain, e HuggingFace.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./app_langchain/routers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./app_langchain/routers.py\n",
    "\n",
    "from fastapi import APIRouter\n",
    "from .models import ModeloPergunta, ModeloTraducao\n",
    "from langchain_community.llms import FakeListLLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Instanciamos o modelo fora da função correspondente pois foi a forma de fazê-lo progredir nas respostas da lista\n",
    "modelo_falso = FakeListLLM(responses=[\n",
    "                                    'Olá, eu sou um robô!',\n",
    "                                    'Estou bem, e você?',\n",
    "                                    'Não sei te dizer...',\n",
    "                                    'Essa é a última resposta da lista.'\n",
    "                                    ]\n",
    "                    )\n",
    "\n",
    "\n",
    "@router.post('/conversar/')\n",
    "async def conversa_falsa(body: ModeloPergunta):\n",
    "    return {'resposta': modelo_falso.invoke(body.pergunta)}\n",
    "\n",
    "\n",
    "@router.post('/traduzir/frances/')\n",
    "async def traduzir_gemini(body: ModeloTraducao):\n",
    "    modelo_traducao_gemini = ChatGoogleGenerativeAI(model='gemini-1.5-flash', api_key=os.getenv('GEMINI_KEY'))\n",
    "    resposta = ChatPromptTemplate([\n",
    "        ('system', 'Translate the english texts to french.'),\n",
    "        ('user', 'Translate the following text: {texto}')\n",
    "    ])\n",
    "\n",
    "    return  {'resposta': modelo_traducao_gemini.invoke(resposta.format_messages(texto=body.texto)).content}\n",
    "\n",
    "\n",
    "@router.post('/traduzir/alemao/')\n",
    "async def traduzir_huggingface(body: ModeloTraducao):\n",
    "    modelo_traducao_huggingface = HuggingFacePipeline.from_model_id(model_id='Helsinki-NLP/opus-mt-en-de', task='translation')\n",
    "\n",
    "    return {'resposta': modelo_traducao_huggingface.invoke(body.texto)}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> http POST localhost:8000/traduzir/alemao/ texto='Please, translate this text.'\n",
    "\n",
    "HTTP/1.1 200 OK\n",
    "content-length: 49\n",
    "content-type: application/json\n",
    "server: uvicorn\n",
    "\n",
    "{\n",
    "    \"resposta\": \"Bitte übersetzen Sie diesen Text.\"\n",
    "}\n",
    "\n",
    "> http POST localhost:8000/traduzir/alemao/ texto='Today is wednesday. What a beautiful day!'\n",
    "\n",
    "HTTP/1.1 200 OK\n",
    "content-length: 61\n",
    "content-type: application/json\n",
    "date: Wed, 13 Nov 2024 21:11:14 GMT\n",
    "server: uvicorn\n",
    "\n",
    "{\n",
    "    \"resposta\": \"Heute ist Mittwoch. Was für ein schöner Tag!\"\n",
    "}\n",
    "\n",
    "![arquitetura_huggingface](Diagrama%20-%20Arquitetura%20HuggingFace.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Questão 4**\n",
    "\n",
    "*Com base na implementação da Questão 2 (Parte 2), explique as principais limitações de utilizar LangChain para integrar a API da OpenAI.*\n",
    "\n",
    "*• Discuta os seguintes aspectos:*\n",
    "\n",
    "*Latência de resposta.*\n",
    "\n",
    "*Limites de uso da API da OpenAI.*\n",
    "\n",
    "*Desafios de escalabilidade e custo.*\n",
    "\n",
    "*Qualidade das traduções geradas em comparação com outros modelos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em relação as respostas obtidas na primeira parte do TP, a aplicação utilizando LangChain teve respostas muito mais demoradas (tanto com o Gemini quanto o modelo via HuggingFace).\n",
    "\n",
    "Como comentado anteriormente, os limites de custo da API da OpenAI também foram um problema. Não foi possível realizar nenhuma consulta gratuita e, portanto, optamos por utilizar o Gemini.\n",
    "\n",
    "Sobre as traduções em si: os resultados foram muito parecidos. Traduções boas de maneira geral, com dificuldades em contextos bem específicos como traduzir algumas expressões idiomáticas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Questão 5**\n",
    "\n",
    "*Com base na aplicação desenvolvida na 3 (Parte 2), explique as limitações de usar LangChain para integrar o modelo HuggingFace de tradução.*\n",
    "\n",
    "*• Discuta aspectos como:*\n",
    "\n",
    "*Desempenho e tempo de resposta.*\n",
    "\n",
    "*Consumo de recursos computacionais.*\n",
    "\n",
    "*Possíveis limitações no ajuste fino do modelo.*\n",
    "\n",
    "*Comparação com o uso direto da API HuggingFace.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma parecida com o aplicativo utilizando Gemini, foi possível observar um tempo de resposta maior ao montar aplicações com o LangChain.\n",
    "\n",
    "Quanto as respostas em si, mesmo se tratando de dois idiomas diferentes (o uso direto do HuggingFace foi de inglês para francês e aqui traduzimos de inglês para alemão) tivemos traduções razoáveis.\n",
    "\n",
    "Abaixo montaremos uma tabela comparando diretamente as duas abordagens.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Questão 6**\n",
    "\n",
    "*Com base nas questões 1-2 (Parte 1) e 2-3 (Parte 2), desenvolva uma tabela comparativa que aborde os seguintes critérios:*\n",
    "\n",
    "*Facilidade de uso/configuração.*\n",
    "\n",
    "*Latência e desempenho.*\n",
    "\n",
    "*Flexibilidade para diferentes modelos.*\n",
    "\n",
    "*Custo e escalabilidade.*\n",
    "\n",
    "*Adequação para protótipos versus aplicações em produção.*\n",
    "\n",
    "*A comparação deve ser apresentada em formato de tabela, com colunas dedicadas a cada critério e linhas comparando FastAPI puro com LangChain.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tabela](Diagrama%20-%20Tabela%20Comparativa.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
